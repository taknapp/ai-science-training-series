{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with consolidating the training routine\n",
    "%%time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class SquareRootScheduler:\n",
    "    def __init__(self, lr=0.1):\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, num_update):\n",
    "        return self.lr * pow(num_update + 1.0, -0.5)\n",
    "\n",
    "def evaluate(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - some NN pieces behave differently during training\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    # We can save computation and memory by not calculating gradients here - we aren't optimizing\n",
    "    with torch.no_grad():\n",
    "        # loop over all of the batches\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            # how many are correct in this batch? Tracking for accuracy\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    accuracy = 100*correct\n",
    "    return accuracy, loss\n",
    "\n",
    "def train_one_epoch(dataloader, model, loss_fn, optimizer, scheduler=None, epoch=1):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # forward pass\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backward pass calculates gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # take one step with these gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # resets the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if scheduler:\n",
    "            if scheduler.__module__ == lr_scheduler.__name__:\n",
    "                # Using PyTorch In-Built scheduler\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                # Using custom defined scheduler\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = scheduler(epoch)\n",
    "\n",
    "class NonlinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layers_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "           # nn.Dropout(0.2),\n",
    "            nn.Linear(50, 10) )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layers_stack(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# read in MNIST data set\n",
    "training_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(training_data))  # 80% for training\n",
    "val_size = len(training_data) - train_size  # Remaining 20% for validation\n",
    "training_data, validation_data = torch.utils.data.random_split(training_data, [train_size, val_size], generator=torch.Generator().manual_seed(55))\n",
    "\n",
    "scheduler = SquareRootScheduler(lr=0.05)\n",
    "\n",
    "nonlinear_model = NonlinearClassifier()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonlinear_model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [32, 64, 128, 256, 512]\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, 10 * pltsize))\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "  # The dataloader makes our dataset iterable\n",
    "  train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "  val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "  epochs = 5\n",
    "  train_acc_all = []\n",
    "  val_acc_all = []\n",
    "\n",
    "  for j in range(epochs):\n",
    "      train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer, scheduler, epoch=j)\n",
    "\n",
    "      # checking on the training loss and accuracy once per epoch\n",
    "      acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "      train_acc_all.append(acc)\n",
    "      #print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "\n",
    "      # checking on the validation loss and accuracy once per epoch\n",
    "      val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "      val_acc_all.append(val_acc)\n",
    "      #print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "  plt.plot(range(epochs), train_acc_all,label = f'Training Acc. ({batch_size})' )\n",
    "  plt.plot(range(epochs), val_acc_all, label = f'Validation Acc. ({batch_size})' )\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "  # The dataloader makes our dataset iterable\n",
    "  train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
    "  val_dataloader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "  epochs = 5\n",
    "  train_acc_all = []\n",
    "  val_acc_all = []\n",
    "\n",
    "  for j in range(epochs):\n",
    "      train_one_epoch(train_dataloader, nonlinear_model, loss_fn, optimizer, scheduler=None, epoch=j)\n",
    "\n",
    "      # checking on the training loss and accuracy once per epoch\n",
    "      acc, loss = evaluate(train_dataloader, nonlinear_model, loss_fn)\n",
    "      train_acc_all.append(acc)\n",
    "      #print(f\"Epoch {j}: training loss: {loss}, accuracy: {acc}\")\n",
    "\n",
    "      # checking on the validation loss and accuracy once per epoch\n",
    "      val_acc, val_loss = evaluate(val_dataloader, nonlinear_model, loss_fn)\n",
    "      val_acc_all.append(val_acc)\n",
    "      #print(f\"Epoch {j}: val. loss: {val_loss}, val. accuracy: {val_acc}\")\n",
    "  plt.plot(range(epochs), train_acc_all,label = f'Training Acc. ({batch_size})' , linestyle='-.')\n",
    "  plt.plot(range(epochs), val_acc_all, label = f'Validation Acc. ({batch_size})' , linestyle='-.')\n",
    "\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot tells us a few things about the impact of increasing batch size and introducing a learning rate scheduler. \n",
    "\n",
    "\n",
    "\n",
    "The most straightforward way of demonstrating the effect of different batch sizes is to train the model on different sized batches and compare. I used batch sizes from 32 to 512. In the session, we used 256. Larger batch sizes tend to take longer to train, thus we only include one example that is larger and a few that are much smaller. We see from the plot above that as batch size increases, the accuracy increases. Not only that, but the variance between the accuracy between the first and last epochs significantly reduces for larger batch sizes. \n",
    "\n",
    "\n",
    "\n",
    "Here, I added a square root learning rate scheduler. This means that the original learning rate ($\\eta _0$) is reduced via $\\eta = \\eta_0 (t + 1)^{1/2}$ where $t$ is the epoch. Overall, the models trained using a learning rate scheduler had higher accuracy than those without.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
